\chapter{RELATED WORK\label{chapter:related_work}}


% Related Work:
% - c2 should be at least 20 pages. Sections:
% -- discuss NLI in general, and methods that range from the classical robust textual entailment (RTE) evaluations, to natural logic, to neural methods.
% -- FV. Discuss the variants that exist from FNC to FEVER. Discuss the methods that have been applied, with a specific focus on neural methods, and how they relate to methods in the previous section.
% -- Domain adaptation techniques. This should cover the history of the field, which is large. Look for a good survey paper and start from there.
% (I can help pointing you to some papers for these sections if you need it)




Recent years have witnessed a boom in research efforts in the NLP field that target machines' capacity to do deep language comprehension. This goes beyond what is clearly expressed in text, instead relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been developed to aid in the development and assessment of such natural language inference ability. In this section we will do a brief review of the history and recent advances in natural language inference.

\section{Natural Language Inference}
Originally the phrase natural language inference was used to refer to statements which needed more contextualized or background knowledge for the machines to understand them. For example consider this example cited in the seminal  \citep{minsky2000commonsense} work on common sense based inference: “Jack needed some money, so he went
and shook his piggy bank. He was disappointed when it made no sound.” This is a statement that is easily understood by humans but not by machines. A human has the necessary background knowledge that piggy bank is a kind of money saving device, and it not having any coins (another form of money) meant that the owner had no savings, and hence is meant to get disappointed. However, it is difficult for machine to comprehend this entire meaning unless it has access to prior knowledge. This, notoriously difficult task for machines, was initially termed as natural language inference. i.e to have an ability to deeply understand natural language beyond what is explicitly expressed. 

 History of NLI is closely entangled with the many benchmark datasets that have been created to help develop and evaluate NLI algorithms and models. These datasets have drawn significant attention from the research community, and many learning
and inference approaches have been developed. Next I will give a brief overview of these datasets, followed by the inference approaches that were developed over them. 

\subsection{Datasets and Methods for NLI}


Creating benchmark datasets to evaluate machines' progress on language processing tasks and promote the development of novel approaches to the natural language inference task has been a long-running task. However, many early attempts were on gathering large-scale annotations to assist data-driven techniques for sub tasks such as part of speech tagging \citep{marcus1993building},
named entity recognition \citep{grishman1996message}, information extraction \citep{sundheim1993tipster}, co-reference resolution and relation extraction \citep{doddington2004automatic}. While these datasets were created primarily to investigate the data contained in linguistic knowledge and context other datasets were created to focus on reading comprehension through tasks like question answering \citep{hirschman1999deep}. To answer questions possible through shallow linguistic approaches, these early reading comprehension benchmarks frequently required only one sentence of context at a time.

In recent years, benchmark datasets (and related tasks) have shifted away from using language context and moved towards requiring a deeper knowledge to complete these tasks. For example consider the sentence from the Winograd schema challenge for evaluating progress in commonsense reasoning 
\citep{morgenstern2015winograd}: “The trophy would not fit in the brown suitcase because it was too big. What was too big?”.
Linguistic constraints alone will not be able to determine if this question relates to the trophy or the brown suitcase. Hence more recent datasets have been built on the belief that successfully solving of these tasks will need machines to go beyond language context and rely on reasoning and information not directly conveyed in text. However, the scope of these benchmark datasets vary from the specific task of co-reference resolution to broader tasks like textual entailment. 
While some benchmarks are focused on a single task (e.g.,coreference resolution task in the Winograd Schema Challenge \citep{morgenstern2015winograd}), others, such as GLUE \citep{wang2018glue}, are made up of a variety of tasks. While some benchmarks are designed to address a single type of knowledge, e.g., commonsense social psychology in Event2Mind  \citep{rashkin2018event2mind}, some others like the CLOZE evaluation for deeper understanding of commonsense stories \citep{mostafazadeh2016corpus} are designed to cover a number of forms of knowledge, which necessitates a wide range of common facts. Some datasets contain multiple-choice questions that require a binary decision or a candidate list selection, while others are more open-ended. Different benchmark features serve different objectives. Therefore determining what criteria to examine in order to establish a benchmark that can assist technological development and provide a quantitative consequence of research progress on deep reasoning abilities is essential. While a comprehensive review of all benchmark tasks that comprise NLI are beyond the scope of this work, in section \ref{sec-rte} we will talk about benchmarks used for a specific sub task very relevant to the current work, Recognizing Textual Entailment \citep{dagan2013recognizing}.

\section{Recognizing Textual Entailment}
\label{sec-rte}


Recognizing Textual Entailment  \citep{dagan2013recognizing} (RTE),  is the task of determining whether the meaning of one text fragment can be inferred (or entailed) from the meaning of the other, given two text fragments. It is also be described as the directed relationship between a text and a hypothesis in which the text entails the hypothesis, if a normal person would conclude that the hypothesis is true based on the text. In section \ref{rte_datasets} we will talk about a few RTE challenges and benchmark datasets which advanced the field by giving rise to certain prominent systems, which will then be described in section \ref{rte_methods} . 

\subsection{RTE Challenges}
\label{rte_datasets}

\todo{add a section for biases in RTE datasets, }

\todo{create a figure showing timeline of creation of RTE challenges }

For many years now, the RTE problems have been the dominant reasoning task under the NLI spectrum\footnote{More recently, especially since the release of Stanford Natural Language Inference dataset \citep{bowman2015large}), the phrase natural language inference is even interchangeably used to denote the task of recognizing textual entailment (RTE).}, inspired by difficulties faced in semantic processing for machine translation, question answering, and information extraction (Dagan et al., 2005). While the initial benchmarks for this task construed of recognizing only \textit{entailment}, some later ones explicitly expected the prediction of \textit{contradiction} also, the opposite label of entailment. Specifically, while in first three RTE challenges \citep{dagan2010recognizing,haim2006second,giampiccolo2007third} the systems were expected to predict whether the text entailed the hypothesis or not, in the fourth and fifth Challenges \citep{bentivogli2009fifth,giampiccolo2008fourth}, systems were also expected to identify contradiction connections between texts and hypotheses as part of a new three-way classification task.
The major objective for the sixth and seventh Challenges \citep{bentivogli2011seventh} were to find entailment given one hypothesis and a corpus with numerous possible entailing phrases.
The eighth challenge \citep{dzikovska2013semeval} focused on categorizing student responses in order to provide automated feedback in an educational context, which was a slightly different challenge.

The Conversational Entailment Dataset \citep{zhang2010towards} which was introduced after this, consisted  of a short series of 875 binary entailment pairs. These consisted of a premise and hypothesis pairs which were created from natural language conversation and hence required the solving of interpreting dialogue. Specifically, many examples required anticipating the beliefs, wishes and intentions of conversation participants that must be explained in a dialog setting.
Sentences Involving Compositional Knowledge (SICK) was another benchmark dataset which had around 10,000 premise hypothesis pairs \citep{marelli2014sick}.
However, there were two subtasks, one to find sentence relatedness and the second one, the classic entailment task. The Stanford Natural Language Inference dataset, introduced in the year 2015, had 60,000 sentence pairs \citep{bowman2015large}. The specific task was similar to the fourth and fifth RTE challenges, \citep{bentivogli2009fifth,giampiccolo2008fourth}, i.e to make a 3-way prediction of these pairs into classes of \textit{entailment}, \textit{neutral} and \textit{contradiction}. Further to these 3 classes, the SNLI dataset also included five labels to denote the agreement between the annotators. Later, this benchmark was expanded to a new dataset named MultiNLI \citep{williams2017broad}, which is in the same structure but includes phrases of many genres including telephone conversations. MultiNLI is available as part of the broader GLUE benchmark \citep{wang2018glue}. SciTail is another entailment dataset that consists of about 27,000 pairs of premise hypotheses tailored to a 2way entailment problem from science questions \citep{khot2018scitail}. This is mainly science-based, and may demand domain-specific expertise, unless alternative benchmarks are used.


\subsection{Methods for Recognizing Textual Entailment}
\label{rte_methods}
\todo{create a figure for basic NN techniques}

A number of techniques have been developed to accomplish the benchmark tasks stated in Section \ref{rte_datasets}. These span from previous symbolic and statistic techniques to the use of more recent deep learning and neural networks. In this section, we will provide a quick review of the early logic based and statistical methods for the benchmarks under consideration, followed by a more thorough explanation of typical neural techniques, which are the current state of the art on all of the benchmarks.

\subsubsection{Logic Based Methods}
\label{rte_methods_logic}


While logic based methods for human reasoning have continued through history from the times of Aristotle, it wasn't until \citep{peirce1883theory} introduced logical abduction that reasonable inference was used for language issues. Logical abduction is the process of reaching a conclusion based on a small number of observations and a small number of assumptions. This is similar to the plausible inference for language problems as defined by \citep{davis2015commonsense}. Meanwhile, other logic theories found use in AI, such as McCarthy's early work on commonsense for machines \citep{mccarthy1960programs}, and linguistics, such as Lakoff's theory of natural logic toward a semantic description of language \citep{lakoff1970linguistics}. Invention of fuzzy logic \citep{zadeh1988fuzzy}, which maps linguistic description of numeric variables to probability distributions over numeric variables, was another significant progress towards managing inaccuracies in human language. Moore \citep{moore1982role} and Nunberg \citep{nunberg1987position} further motivated the importance of reasoning in AI and the role of logic in common sense reasoning. Symbolic approaches to the representation of knowledge and the semantic processing of language were also prevalent throughout the early 1990s \citep{birnbaum1991rigor}. As a consequence, these techniques, especially symbolic approaches, were utilized extensively for early NLI challenges and later for RTE challenges. For example in \citep{raina2005robust} sentences are parsed in a logical manner and then performed abduction over them using learned assumptions and probabilities to establish if a very similar set of assumptions may be utilized to demonstrate that a statement entails another. In another similar work Giampiccolo \citep{giampiccolo2008fourth} supplemented information in the hypothesis sentence with outside semantic knowledge from resources such as Wikipedia, WordNet \citep{miller1995wordnet}, and VerbOcean \citep{chklovski2004verbocean}, then attempted to map this to terms in the premise phrase using manually created logic rules. In a subsequent work \citep{maccartney2007natural} turned natural logic into a formalism for NLI, parsing the premise and hypothesis into natural logic form and utilizing a decision tree to compare features and make a decision. On the third RTE Challenge dataset \citep{giampiccolo2007third}, they demonstrate that combining this technique with an existing statistical RTE model outperforms the state-of-the-art performance. Another similar technique that yields high performance develops a comprehensive collection of hand produced logic and commonsense rules to apply with supplied mappings from plain language in benchmark data to logical forms \citep{gordon2016commonsense}. While it has been demonstrated to be extremely efficient in some tasks to manually draw up logical rules and mappings from a language to a logical form, this cannot be scaled for more modern large datasets, in which there is more variation in required knowledge and language and much higher semantic phenomena. To mitigate this \citep{kamath2018survey} developed certain techniques, known as semantic parsing, which automatically map natural language text to a logical form, in order to allow more practical, scalable, logic based reasoning.

\subsubsection{Statistical Methods}
\label{rte_methods_stat}


From mid-1990s till early 2010 many statistical techniques dominated the NLP field. These earlier statistical techniques relied on engineered features to train various types of statistical models based on training data for a number of early NLP benchmarks. For example, early methods frequently coupled different word matching and other lexical features with traditional statistical models such as decision trees \citep{ng2000machine} or with hand crafted deterministic rules \citep{charniak2000reading}.

Similar techniques were used in several of the RTE benchmarks, beginning with the RTE Challenges \citep{dagan2005pascal}. Lexical characteristics based on bag-of-words and word matching, for example, were frequently utilized in the first two RTE challenges \citep{dagan2005pascal,haim2006second}, but yielded only marginally better outcomes than random guessing. Additionally, to generate predictions some other systems have employed more linguistic features, such as, hidden correlation biases in benchmark data, synonym, antonym, and hypernym relationships derived from training data \citep{lai2014illinois}, semantic dependency and paraphrases \citep{hickl2006recognizing}. Another category of solutions use external data sources and the Internet to augment training data characteristics. For instance, a naive Bayes classification system utilizing features from the co-occurrences of word on a web search engine \citep{glickman2006applied} was the best system in the initial RTE Challenge \citep{dagan2005pascal}. In order to assess the statistical measure of entailment between the given phrases, the top system for the seventh RTE Challenge \citep{tsuchida2011ikoma} also utilised some external knowledge resources such as WordNet \citep{miller1995wordnet} and CatVar \citep{habash2003catvar} along with the acronyms produced from the training data. Even though the use of some external knowledge bases have an advantage over models that simply employ language characteristics derived from the data, they have not been competitive enough in the recent benchmarks of enormous data size. Nevertheless, as demonstrated by \citep{zhang2017ordinal}, they continue to be used as useful baselines for new benchmarks. 



\subsubsection{Neural Methods}
\label{rte_methods_neural}
 Aided by many factors, including but not limited to, the Internet, the rising amount of data for current benchmarks, acceleration in software and hardware discoveries etc., larger and deeper neural networks are being invented and trained day by day. As of writing this work, neural network based solutions are the state of the art in most of the NLI benchmarks discussed so far. Next, we will elaborate on some common techniques and architectures that contributed to this journey.
 
 
 
Neural methods owe their origins to past statistical approaches used for predictions. However, instead of manually describing all the features like the past statistical approaches did, it utilizes different types of architectures to find valuable features in the data. One such feature recognition method, a crucial method that accelerated the rise of neural networks, was the invention of the distributional representation of words like Word2vec \citep{mikolov2017advances} and Glove \citep{pennington2014glove}. The word vectors (also known as embeddings) representing features are frequently learned (and in some cases updated) from large text corpora using neural networks. However, one shortcoming of these early approaches were that the embedding vector of the target word is always the same, regardless of the context in which it occurs. As a result, these embeddings are incapable of representing distinct word meanings in different contexts, despite the fact that this is a common occurrence in language. Recent works, such as Embeddings from Language Models (ELMO)\citep{peters2018deep} and Bidirectional Encoder Representations (BERT) \citep{devlin2018bert} from Transformers \citep{vaswani2017attention}, have created contextual word representation models to solve this challenge. These approaches assign various embedding vectors to words depending on their context. 


These pre-trained word representations can be fine-tuned for downstream tasks or utilized as features. For example, the Generative Pre-trained Transformer (GPT) \citep{radford2018improving} and BERT  \citep{devlin2018bert}, introduce few task-specific parameters and may be fine-tuned on downstream tasks with modified final layers and loss functions. Further, task-specific network architectures are built for various downstream applications on top of these word embedding layers. These task-specific architectures used to handle specific tasks frequently include recurrent neural networks (RNNs) such as LSTMs \citep{hochreiter1997long} and GRUs \citep{cho2014properties} convolutional neural networks (CNNs), or more recently, transformers\citep{vaswani2017attention}.

These networks' output layers are chosen based on the task specification. A linear layer and softmax, for example, are frequently used for classification, whereas a language decoder is frequently used for language creation. RNN-based architectures are extensively and frequently used in both baseline \citep{bowman2015large,rashkin2018event2mind} and state-of-the-art techniques \citep {kim2019semantic,chen2018hfl,henaff2016tracking} because of the sequential structure of language. Neural models benefit from approaches like attention mechanisms and memory augmentation when they have diverse topologies. Memory-augmented networks, such as memory networks \citep{weston2014memory} and recurrent entity networks \citep{henaff2016tracking}, have been proven to be successful for tasks  that require reasoning based on numerous supporting facts (e.g., bAbI \citep{weston2015towards}).


\pagebreak

\todo{starting new page here on july 13th 2011}

Since its first usage in neural machine translation \citep{bahdanau2014neural}, attention has been frequently employed in NLP tasks. Next we will elaborate on a few  neural models for the aforementioned benchmarks in question, with a particular focus on those based on the attention mechanism, since they comprise most of the past and current state-of-the-art models on these benchmarks. 

\subsubsection{Attention Based Neural Methods}: 

Attention is a mechanism which was first used to capture the alignment between an input (encoder) and an output (decoder). In this mechanism, the decoder chooses which portions of the original phrase to pay attention to. Thus by allowing the decoder to have an attention mechanism the encoder is alleviated from the burden of needing to encode every information in the source phrase into a fixed length vector. With this novel technique, information may be dispersed across the sequence of annotations, and the decoder can extract it selectively as needed. 

There are numerous advantages to modeling attention. Like mentioned above it enables the decoder to go straight to specific sections of the input and focus on them. It solves the vanishing gradient problem by allowing for states further in the input sequence to be accounted for. Another advantage is that the model's learned attention allocation automatically aligns inputs and outputs, allowing some comprehension of their relationships.

Adding an attention mechanism to RNNs, LSTMs, CNNs, and other deep learning models has been demonstrated to enhance performance on a variety of tasks when compared to their vanilla counterparts \citep{kim2019semantic}. It works especially well for tasks that need input and output alignment, such as different textual entailment tasks that require context and hypothesis modeling. One of the best performing systems in the SNLI challenge, for example, employs a densely connected RNN while concatenating features from an attention mechanism to recurrent features in the network \citep{kim2019semantic}. The attention weights that arise from this alignment, as explained by \citep{kim2019semantic}, aid the system in making correct entailment and contradiction judgements for very similar pairs of phrases. One such example they give is when the hypothesis statement "Several individuals in front of a gray building" is compared to the context sentence "Several guys in front of a white building."

In contrast to the aforementioned architectures which add attention mechanisms to neural architectures like CNN, RNN etc., is the recently proposed Transformer architecture which is entirely composed of attention mechanisms \citep{vaswani2017attention}. The self-attention layer in both the encoder and the decoder is the fundamental distinction in Transformer networks. Self-attention permits it to attend to all positions in an input sequence to better encode the word. It provides a mechanism for possibly capturing long-term word dependencies such as syntactic, semantic, and coreference relations. Furthermore, rather than performing a single attention function, the transformer performs multi-head attention. i.e., it applies the attention function multiple times with different linear projections. This allows the model to jointly capture different attentions from different subspaces, e.g., jointly attend to information that may indicate both coreference and syntactic relation. Another significant advantage of the transformer is its capacity to support parallel processing. Because of their sequential nature, sequence models such as RNN and LSTM are challenging to parallelize. The transformer model meanwhile, by using attention to capture global relationships between inputs and outputs, maximizes the number of parallelizable computations. Further, Transformers have lately been employed in pre-trained contextual models like \citep{radford2018improving} and BERT  \citep{devlin2018bert} to attain state-of-the-art performance on several of the RTE benchmarks mentioned earlier.